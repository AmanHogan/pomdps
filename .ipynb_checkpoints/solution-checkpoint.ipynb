{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.a**\n",
    "\n",
    "For fully observable navigation, the state space should include all the necessary information for the agent to make decisions. This includes position and orientation. So the state space is a tuple such that:\n",
    "\n",
    "$statespace = (x_i,y_j,o_k)$ for all $i,j,k$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x_i$ is x position in range 10\n",
    "\n",
    "- $y_j$ is y position in range 20\n",
    "\n",
    "- $o_k$ is orientation in where $o_k \\epsilon [up,right,left,down]$\n",
    "\n",
    "\n",
    "For fully observable navigation, the action space should be the movements that only the agent to traverse the grid.\n",
    "So the action space combination of actions the agent can take.\n",
    "\n",
    "$actionspace = ['turn_{right}, turn_{left}, move_{forward}, move_{backward}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1.b**\n",
    "\n",
    "The simulator has been implemented. Below are links to the simulator/envirinment implementation:\n",
    "\n",
    "- **[Fully Observable Environment](environment.py#L11)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.c**\n",
    "\n",
    "The Q learner has been implmented and can be found here:\n",
    "\n",
    "- **[Q Learning Agent](learners.py#L9)**\n",
    "\n",
    "It runs alongside the POMDP agents. You can run the agents with default parameters by typing in : `python -m agents`\n",
    "\n",
    "However, you can run the program with different paramaters like the given example below:\n",
    "\n",
    "`python -m agents --start_state \"(1, 1, 'UP')\" --episode_num 25 --episode_length 1700 --alpha 0.01 --gamma 1.0 --epsilon 0.35 --num_obstacles 5 --trials 4`\n",
    "\n",
    "Once the program finishes, a logfile.txt is provided to check completion time, and the graphs are saved that show the epsiode v rewards. \n",
    "\n",
    "### **Results**\n",
    "\n",
    "I ran the program with the following paramaters:\n",
    "\n",
    "2024-04-13 16:14:50: {'start_state': \"(1, 1, 'UP')\", \n",
    "'episode_num': 25, 'episode_length': 2000, 'alpha': 0.1, \n",
    "'gamma': 1.0, 'epsilon': 0.3, 'num_obstacles': 5, 'trials': 4}\n",
    "\n",
    "\n",
    "There were four trials with randomized goals and obstacles:\n",
    "\n",
    "**TRIAL 1**\n",
    "- OBSTACLES:[(8, 14), (6, 14), (5, 1), (1, 17), (2, 5)]\n",
    "- GOAL:(7, 4)\n",
    "\n",
    "**TRIAL 2**\n",
    "- OBSTACLES:[(2, 14), (10, 4), (10, 19), (7, 5), (10, 2)]\n",
    "- GOAL:(6, 5)\n",
    "\n",
    "**TRIAL 3**\n",
    "- OBSTACLES:[(2, 17), (6, 20), (1, 10), (3, 12), (5, 9)]\n",
    "- GOAL:(6, 2)\n",
    "\n",
    "**TRIAL 4**\n",
    "- OBSTACLES:[(4, 10), (6, 4), (2, 3), (4, 11), (10, 2)]\n",
    "- GOAL:(9, 6)\n",
    "\n",
    "\n",
    "Below is the graph for the **Q Learner** I ran and the respective 4 different trials:\n",
    "\n",
    "![img](./graphs/Q_Learning_num_eps_25.png)\n",
    "\n",
    "\n",
    "Each trial performed as expected. Trial 3 did the best overall likely because the goal (6,2) was the closest to the start state. Trial 4 did the worst likely because it was the furthest (9,6) from the start state. All trials evetually performed better over the runtime of the program.\n",
    "\n",
    "- 2024-04-13 16:15:00: Trial: 1. Finished Q Learning. Goal finding efficiency: 0.88\n",
    "- 2024-04-13 16:17:24: Trial: 2. Finished Q Learning. Goal finding efficiency: 0.96\n",
    "- 2024-04-13 16:21:34: Trial: 3. Finished Q Learning. Goal finding efficiency: 0.88\n",
    "- 2024-04-13 16:25:09: Trial: 4. Finished Q Learning. Goal finding efficiency: 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a\n",
    "\n",
    "\n",
    "Now we have an POMDP. The agent doesnt have access to the true state of the environment. It only knows if has hit a wall, obstacle, goal, or none of the options. So to design an observation fucntion, we need to design it in the environemnt. This **observation function** will 'filter' the true state and return the what the agent sees.\n",
    "\n",
    "$observation = Observation_{func}(state)$\n",
    "\n",
    "In the below code, the environemnt has the true state and returns the `observed_state` to the agent. In practice, this should mimic how an agent would only obtain certain infromation from its sensors. Here is the **observation function**\n",
    "\n",
    "```\n",
    "    def observation_function(self, next_state):\n",
    "\n",
    "        x_2, y_2, orien_2 = next_state\n",
    "        observed_state = None\n",
    "\n",
    "        # Return observation from agent's view, given the actual state\n",
    "        if (x_2, y_2)in self.obstacles:\n",
    "            observed_state = HIT_OBSTACLE\n",
    "\n",
    "        elif (x_2,y_2) == self.goal_state:\n",
    "            observed_state = HIT_GOAL\n",
    "    \n",
    "        elif x_2 > X_DIRECTION or y_2 > Y_DIRECTION or x_2 < 1 or  y_2 < 1:\n",
    "            observed_state = HIT_WALL\n",
    "\n",
    "        else:\n",
    "            observed_state = HIT_NONE\n",
    "           \n",
    "        return observed_state\n",
    "\n",
    "```\n",
    "\n",
    "On top of the observation function, we need an observation probability function to obtain $O$, or the set opf conditional observation probabilities. Since we assumed the environment was deterministic, action $a$ has a 100% of leading to state $s'$ from state $s$, our **observation probability function** becomess much more simple; there is only one possible observation for every state-action pair.\n",
    "\n",
    "So out **observation probability** function becomes:\n",
    "\n",
    "```\n",
    "def observation_probs(self, state, observation, action):\n",
    "    chance_correct = 1\n",
    "    chance_wrong = 1 - chance_correct\n",
    "    expected = self.observation_function(state)\n",
    "\n",
    "    if observation == expected:\n",
    "        return chance_correct\n",
    "    else:\n",
    "        return chance_wrong\n",
    "```\n",
    "\n",
    "Later on, when we caluclate belief updates, this simplifies our calculations since traditionally the belief update is:\n",
    "\n",
    "$b'(s') = n O(o|s',a) \\sum T(s'|s,a) b(s)$\n",
    "\n",
    "But since the environment is deterministic, the probability that an observation matches the expected outcome given a state and action is effectively 1 and, conversely, the probability of any other observation is 0. Now making the **belief update**:\n",
    "\n",
    "$b'(s') = n  \\sum T(s'|s,a) b(s)$\n",
    "\n",
    "Here is the link to my functions:\n",
    "\n",
    "**[Observations](environment.py#L272)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b\n",
    "\n",
    "The code has been expanded to handle observations and we added some new functions to do this:\n",
    "```\n",
    "def observation_probs(self, state, observation, action):\n",
    "def observation_function(self, next_state):\n",
    "def init_belief(start_state):\n",
    "def update_belief(observation, agent):\n",
    "def compute_Q_b_a(action, agent):\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c\n",
    "\n",
    "### **Results**\n",
    "\n",
    "I ran the program with the following paramaters:\n",
    "\n",
    "2024-04-13 16:14:50: {'start_state': \"(1, 1, 'UP')\", \n",
    "'episode_num': 25, 'episode_length': 2000, 'alpha': 0.1, \n",
    "'gamma': 1.0, 'epsilon': 0.3, 'num_obstacles': 5, 'trials': 4}\n",
    "\n",
    "\n",
    "There were four trials with randomized goals and obstacles:\n",
    "\n",
    "**TRIAL 1**\n",
    "- OBSTACLES:[(8, 14), (6, 14), (5, 1), (1, 17), (2, 5)]\n",
    "- GOAL:(7, 4)\n",
    "\n",
    "**TRIAL 2**\n",
    "- OBSTACLES:[(2, 14), (10, 4), (10, 19), (7, 5), (10, 2)]\n",
    "- GOAL:(6, 5)\n",
    "\n",
    "**TRIAL 3**\n",
    "- OBSTACLES:[(2, 17), (6, 20), (1, 10), (3, 12), (5, 9)]\n",
    "- GOAL:(6, 2)\n",
    "\n",
    "**TRIAL 4**\n",
    "- OBSTACLES:[(4, 10), (6, 4), (2, 3), (4, 11), (10, 2)]\n",
    "- GOAL:(9, 6)\n",
    "\n",
    "\n",
    "Below is the graph for the $Q_{MDP}$. I ran and the respective 4 different trials:\n",
    "\n",
    "\n",
    "![img](./graphs/QMDP_Learning_num_eps_25.png)\n",
    "\n",
    "\n",
    "The $Q_{MDP}$, despite not being a full POMDP because its a hybrid, did relatively well at aproximating the POMPD values. Unlike the Q original learner in the fully observable mdp, the $Q_{MDP}$ has less variance in the graph. It was also much better at learning and had several epsiodes in a row that near perfect 100 reward. This is likely because it had access to the previous MDP's $q(s,a)$ paramaters\n",
    "\n",
    "- 2024-04-13 16:17:17: Trial: 1. Finished Starting QMDP Learning. Goal finding efficiency: 1.0\n",
    "- 2024-04-13 16:21:27: Trial: 2. Finished Starting QMDP Learning. Goal finding efficiency: 0.64\n",
    "- 2024-04-13 16:24:54: Trial: 3. Finished Starting QMDP Learning. Goal finding efficiency: 0.68\n",
    "- 2024-04-13 16:30:49: Trial: 4. Finished Starting QMDP Learning. Goal finding efficiency: 0.72\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d\n",
    "\n",
    "I ran the program with the following paramaters:\n",
    "\n",
    "2024-04-13 16:14:50: {'start_state': \"(1, 1, 'UP')\", \n",
    "'episode_num': 25, 'episode_length': 2000, 'alpha': 0.1, \n",
    "'gamma': 1.0, 'epsilon': 0.3, 'num_obstacles': 5, 'trials': 4}\n",
    "\n",
    "\n",
    "There were four trials with randomized goals and obstacles:\n",
    "\n",
    "TRIAL 1\n",
    "- OBSTACLES:[(8, 14), (6, 14), (5, 1), (1, 17), (2, 5)]\n",
    "- GOAL:(7, 4)\n",
    "\n",
    "TRIAL 2\n",
    "- OBSTACLES:[(2, 14), (10, 4), (10, 19), (7, 5), (10, 2)]\n",
    "- GOAL:(6, 5)\n",
    "\n",
    "TRIAL 3\n",
    "- OBSTACLES:[(2, 17), (6, 20), (1, 10), (3, 12), (5, 9)]\n",
    "- GOAL:(6, 2)\n",
    "\n",
    "TRIAL 4\n",
    "- OBSTACLES:[(4, 10), (6, 4), (2, 3), (4, 11), (10, 2)]\n",
    "- GOAL:(9, 6)\n",
    "\n",
    "\n",
    "Below is the graph for Replicated Q Learning the I ran and the respective 4 different trials:\n",
    "\n",
    "![img](./graphs/Replicated_Q_Learning_num_eps_25.png)\n",
    "\n",
    "Unlike the $Q_{MPD}$, the replicated q learner did not have access to the $q(s,a)$ paramaters, so it had to learn in a completely POMDP environment. Despite this, it had pretty solid performance. It had a perfect 100 reward for several epsidoes in a row. At epsidoe 17 it ended up hadving -600 reward, but overall had a pretty solid learning curve.\n",
    "\n",
    "- 2024-04-13 16:16:51: Trial: 1. Finished Replicated Learning Q Learning. Goal finding efficiency: 1.0\n",
    "- 2024-04-13 16:19:44: Trial: 2. Finished Replicated Learning Q Learning. Goal finding efficiency: 0.92\n",
    "- 2024-04-13 16:23:19: Trial: 3. Finished Replicated Learning Q Learning. Goal finding efficiency: 0.92\n",
    "- 2024-04-13 16:29:06: Trial: 4. Finished Replicated Learning Q Learning. Goal finding efficiency: 0.68\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.e\n",
    "\n",
    "I ran the program with the following paramaters:\n",
    "\n",
    "2024-04-13 16:14:50: {'start_state': \"(1, 1, 'UP')\", \n",
    "'episode_num': 25, 'episode_length': 2000, 'alpha': 0.1, \n",
    "'gamma': 1.0, 'epsilon': 0.3, 'num_obstacles': 5, 'trials': 4}\n",
    "\n",
    "\n",
    "There were four trials with randomized goals and obstacles:\n",
    "\n",
    "TRIAL 1\n",
    "- OBSTACLES:[(8, 14), (6, 14), (5, 1), (1, 17), (2, 5)]\n",
    "- GOAL:(7, 4)\n",
    "\n",
    "TRIAL 2\n",
    "- OBSTACLES:[(2, 14), (10, 4), (10, 19), (7, 5), (10, 2)]\n",
    "- GOAL:(6, 5)\n",
    "\n",
    "TRIAL 3\n",
    "- OBSTACLES:[(2, 17), (6, 20), (1, 10), (3, 12), (5, 9)]\n",
    "- GOAL:(6, 2)\n",
    "\n",
    "TRIAL 4\n",
    "- OBSTACLES:[(4, 10), (6, 4), (2, 3), (4, 11), (10, 2)]\n",
    "- GOAL:(9, 6)\n",
    "\n",
    "\n",
    "Below is the graph for Linear Q Learning the I ran and the respective 4 different trials:\n",
    "\n",
    "![img](./graphs/Linear_Q_Learning_num_eps_25.png)\n",
    "\n",
    "Unlike the $Q_{MPD}$, the linear q learner did not have access to the $q(s,a)$ paramaters, so it had to learn in a completely POMDP environment. The learner had solid performance at the begining, bad performance halfway, then good perfance at the end. The graphs over all trials kind of make a 'U' shape. There was solid performance all around except for trial 4, which is likely because of that goal being located farther away from the start state than the others.\n",
    "\n",
    "- 2024-04-13 16:16:41: Trial: 1. Finished Linear Q Learning. Goal finding efficiency: 0.68\n",
    "- 2024-04-13 16:19:13: Trial: 2. Finished Linear Q Learning. Goal finding efficiency: 0.64\n",
    "- 2024-04-13 16:22:32: Trial: 3. Finished Linear Q Learning. Goal finding efficiency: 0.88\n",
    "- 2024-04-13 16:27:35: Trial: 4. Finished Linear Q Learning. Goal finding efficiency: 0.44\n",
    "\n",
    "\n",
    "### Comparing POMDP Agents\n",
    "\n",
    "#### Similarities\n",
    "Every learner did well on trial 3.\n",
    "Every learner did well on trial 2.\n",
    "\n",
    "#### Differences\n",
    "Replicated Q learning had the best performance.\n",
    "Linear Q learning had the worst performance between all POMDP learners.\n",
    "$Q_{MDP}$ had the lowest reward minimum.\n",
    "\n",
    "#### Conclusions\n",
    "Linear Q learning seemed to be unstable on the episode number I used, replicated learning seemed to perform the best, and QMDP had good middle ground performance. In parially observable environments, replicated q learning would be the option I would use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
